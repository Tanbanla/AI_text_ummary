!pip install torch torchvision torchaudio
!pip install transformers datasets
#
!pip install transformers datasets vncorenlp flask
#
from datasets import load_dataset

dataset = load_dataset('cnn_dailymail', '3.0.0')
train_data = dataset['train']
valid_data = dataset['validation']
test_data = dataset['test']
#
from transformers import BartTokenizer

tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')

def tokenize_function(example):
    return tokenizer(example['article'], max_length=1024, truncation=True)

train_data = train_data.map(tokenize_function, batched=True)
valid_data = valid_data.map(tokenize_function, batched=True)
test_data = test_data.map(tokenize_function, batched=True)
#
from transformers import BartForConditionalGeneration

model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')
#
from torch.utils.data import DataLoader

train_dataloader = DataLoader(train_data, batch_size=8, shuffle=True)
valid_dataloader = DataLoader(valid_data, batch_size=8)
#
from transformers import AdamW, get_scheduler

# Giả sử bạn đã định nghĩa model và train_dataloader trước đó
# model = ...
# train_dataloader = ...

# Định nghĩa số lượng epochs
num_epochs = 3  # Ví dụ: 3 epochs

# Tạo optimizer
optimizer = AdamW(model.parameters(), lr=5e-5)

# Tính toán số lượng bước huấn luyện
num_training_steps = len(train_dataloader) * num_epochs

# Tạo scheduler
lr_scheduler = get_scheduler(
    name="linear", 
    optimizer=optimizer, 
    num_warmup_steps=0, 
    num_training_steps=num_training_steps
)

# Tiếp tục với quá trình huấn luyện...
#
import torch
from tqdm.auto import tqdm
from torch.utils.data import DataLoader
from transformers import BartTokenizer, BartForConditionalGeneration, get_scheduler

# Tải tokenizer và mô hình BART
tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')
model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')

# Ví dụ về dữ liệu đầu vào
texts = ["Hello, how are you?", "I am fine, thank you!", "This is a longer piece of text to demonstrate padding."]
labels = ["Hi, how do you do?", "I'm good, thanks!", "This is a longer text to show padding."]

# Mã hóa văn bản
inputs = [tokenizer(text, return_tensors='pt') for text in texts]
labels = [tokenizer(label, return_tensors='pt') for label in labels]

# Định nghĩa collate function tùy chỉnh
def collate_fn(batch):
    input_ids = [item['input_ids'].squeeze(0) for item in batch]
    attention_mask = [item['attention_mask'].squeeze(0) for item in batch]
    labels = [item['labels'].squeeze(0) for item in batch]
    
    # Thực hiện padding cho input_ids, attention_mask và labels
    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)
    attention_mask = torch.nn.utils.rnn.pad_sequence(attention_mask, batch_first=True, padding_value=0)
    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)
    
    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}

# Kết hợp input và labels
data = [{'input_ids': inp['input_ids'], 'attention_mask': inp['attention_mask'], 'labels': lbl['input_ids']} for inp, lbl in zip(inputs, labels)]

# Tạo DataLoader
train_dataloader = DataLoader(data, batch_size=2, collate_fn=collate_fn)

# Định nghĩa số lượng epochs
num_epochs = 3

# Tạo optimizer sử dụng PyTorch AdamW
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)

# Tính toán số lượng bước huấn luyện
num_training_steps = len(train_dataloader) * num_epochs

# Tạo scheduler
lr_scheduler = get_scheduler(
    name="linear", 
    optimizer=optimizer, 
    num_warmup_steps=0, 
    num_training_steps=num_training_steps
)

# Sử dụng thiết bị CUDA nếu có sẵn
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)

# Tạo progress bar
progress_bar = tqdm(range(num_training_steps))

# Huấn luyện mô hình
model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        inputs = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**inputs)
        loss = outputs.loss
        if loss is not None:
            loss.backward()

            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            progress_bar.update(1)

print("Training complete!")
#
!pip install --upgrade transformers safetensors
#
from transformers import BartForConditionalGeneration, BartTokenizer, GenerationConfig

# Tạo mô hình và tokenizer từ pretrained
model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')
tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')

# Tạo GenerationConfig với các tham số tùy chỉnh
generation_config = GenerationConfig(
    max_length=142,
    min_length=56,
    early_stopping=True,
    num_beams=4,
    length_penalty=2.0,
    no_repeat_ngram_size=3,
    forced_bos_token_id=0,
    forced_eos_token_id=2
)

# Lưu mô hình, tokenizer, và GenerationConfig mà không dùng safe_serialization
model.save_pretrained("./summarization_model", safe_serialization=False)
tokenizer.save_pretrained("./summarization_model")
generation_config.save_pretrained("./summarization_model")

print("Model, tokenizer, and GenerationConfig saved successfully!")
#
from transformers import BartForConditionalGeneration, BartTokenizer, GenerationConfig

# Tải mô hình, tokenizer, và GenerationConfig từ thư mục lưu
model = BartForConditionalGeneration.from_pretrained("./summarization_model")
tokenizer = BartTokenizer.from_pretrained("./summarization_model")
generation_config = GenerationConfig.from_pretrained("./summarization_model")

# Ví dụ văn bản để tóm tắt
article = "Trong thung lũng yên bình của ngôi làng nhỏ, có một đàn cừu sống hạnh phúc dưới sự bảo vệ của chú chó canh gác dũng cảm. Mỗi ngày, chúng lang thang trên những đồng cỏ xanh mướt, không hề biết rằng, đôi mắt của kẻ săn mồi đang dõi theo từ bóng tối rậm rạp của khu rừng. Sói, với bản năng săn mồi của mình, luôn tìm cách tiếp cận đàn cừu, nhưng chú chó canh gác luôn sẵn sàng để bảo vệ đàn cừu của mình.Một hôm, khi mặt trời lặn sau ngọn đồi, sói đã lẻn vào thung lũng và tiến gần đến đàn cừu. Nhưng chú chó canh gác đã phát hiện ra kẻ xâm nhập và lập tức đuổi theo. Cuộc rượt đuổi diễn ra quyết liệt, với tiếng sủa vang dội khắp thung lũng, khiến sói phải bỏ chạy vào rừng sâu. Đàn cừu được an toàn, và chú chó trở thành người hùng trong mắt của cả làng.Câu chuyện này không chỉ là một câu chuyện giản dị về cuộc sống nông thôn, mà còn là một bài học về lòng dũng cảm và tình bạn. Chú chó canh gác không chỉ là một vệ sĩ, mà còn là một người bạn đáng tin cậy của đàn cừu. Sự kiên trì và lòng trung thành của chú đã chứng minh rằng, dù có khó khăn đến đâu, tình bạn và sự bảo vệ luôn có thể chiến thắng mọi thử thách. Và sói, dù mạnh mẽ và tinh ranh, cũng phải nhận ra rằng, sức mạnh không phải lúc nào cũng là chìa khóa để chiến thắng. Đôi khi, sự thông minh và lòng dũng cảm mới là những phẩm chất quyết định. Câu chuyện của chú chó canh gác và đàn cừu đã trở thành huyền thoại, được lưu truyền qua nhiều thế hệ, nhắc nhở chúng ta về giá trị của lòng trung thành và tầm quan trọng của việc bảo vệ những người yếu thế."

# Tokenize input
inputs = tokenizer(article, return_tensors="pt", max_length=1024, truncation=True)

# Sinh tóm tắt
summary_ids = model.generate(
    inputs['input_ids'],
    generation_config=generation_config  # Sử dụng GenerationConfig đã tải
)

# Giải mã tóm tắt
summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

print("Summary:", summary)